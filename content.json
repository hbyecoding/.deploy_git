{"meta":{"title":"Hongbo Website","subtitle":"","description":"personal blog website","author":"hongbo","url":"https://hbyecoding.github.io","root":"/"},"pages":[{"title":"","date":"2023-11-09T14:22:03.196Z","updated":"2023-11-09T14:22:03.177Z","comments":true,"path":"StatisticalLearnign2.html","permalink":"https://hbyecoding.github.io/StatisticalLearnign2.html","excerpt":"","text":"李航之ch2 感知机二分类的线性分类模型page25 感知机page27 数据集的线性可分型 误分类的数据$x_i$ 到超平面的距离 求和 可以得到损失函数 2.3 学习算法-随机梯度下降法page30 原始算法 依据收敛性 到新的算法(dual) 感知聚学习算法的对偶形式"}],"posts":[{"title":"test_2023_11_20","slug":"test-2023-11-20","date":"2023-11-20T02:39:33.000Z","updated":"2023-11-20T02:41:44.293Z","comments":true,"path":"2023/11/20/test-2023-11-20/","link":"","permalink":"https://hbyecoding.github.io/2023/11/20/test-2023-11-20/","excerpt":"","text":"Just for test!you need some package before you give your ideas.","categories":[],"tags":[{"name":"test","slug":"test","permalink":"https://hbyecoding.github.io/tags/test/"}]},{"title":"","slug":"StatisticalLearning4","date":"2023-11-09T14:30:05.429Z","updated":"2023-11-09T14:30:06.050Z","comments":true,"path":"2023/11/09/StatisticalLearning4/","link":"","permalink":"https://hbyecoding.github.io/2023/11/09/StatisticalLearning4/","excerpt":"","text":"李航之ch4 朴素贝叶斯法","categories":[],"tags":[]},{"title":"","slug":"StatisticalLearning3","date":"2023-11-09T14:30:05.306Z","updated":"2023-11-09T14:30:05.757Z","comments":true,"path":"2023/11/09/StatisticalLearning3/","link":"","permalink":"https://hbyecoding.github.io/2023/11/09/StatisticalLearning3/","excerpt":"","text":"李航之ch3 k邻近法page39 ~ kd tree page413 elements: 距离度量, k值的选择 和分类决策规则","categories":[],"tags":[]},{"title":"","slug":"StatisticalLearning1","date":"2023-11-09T14:30:05.176Z","updated":"2023-11-09T14:30:05.460Z","comments":true,"path":"2023/11/09/StatisticalLearning1/","link":"","permalink":"https://hbyecoding.github.io/2023/11/09/StatisticalLearning1/","excerpt":"","text":"李航之ch1统计学习方法概论统计学习 监督学习统计机器学习(statistical machine learning) 建立在计算机及网络之上的, 数据驱动的, 目的是对数据预测和分析的, 构建了模型并且应用模型预测和分析的, 应用到了概率论, 统计学, 计算理论,最优化理论和计算机信息论的. 方法 supervised learning(*) nusupervised learning semi-supervised learning reinforcement learning 基本概念输入空间特征空间= 特征向量存在的空间.每一维度对应一个特征. (有时候这两个空间等同,有时候不一样)输出空间联合概率分布假设空间 统计学习三要素 模型 策略和算法方法 = 模型 + 策略 + 算法1 策略, 需要首先损失函数与风险函数的概念, loss 度量模型一次预测的好坏, 风险函数度量平均意义下的模型预测的好坏2 经验风险最小化与结构风险最小化经验风险最小化 能保证有很好的学习效果.结构风险最小化 类似于 最优化. 正则化和交叉验证 指示函数 Ir + e = 1 泛化能力generalization abilitygeneralization error (less , better)generalization error bound (less , better)function(capacity) 假设空间容量越大,模型就越难学, 泛化误差上界就大page 15 Hoeffding unequation 生成模型generative model 与判别模型 discriminativepage18 生成方法的特点, 判别方法的特点 分类问题page19 回归问题","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2023-11-09T14:30:00.657Z","updated":"2023-11-09T14:30:00.950Z","comments":true,"path":"2023/11/09/hello-world/","link":"","permalink":"https://hbyecoding.github.io/2023/11/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. hongbo’s first hexo blogfirst title哇得奥得啊无123import numpy as npa = np.array([1,2,3])b = np.array([1,2,3]) hello world \\sum_{i=1}^\\infty\\frac{1}{i^2}=\\frac{\\pi^2}{6}","categories":[],"tags":[]},{"title":"","slug":"2023_09_27_del","date":"2023-11-09T14:29:59.398Z","updated":"2023-11-09T14:29:59.675Z","comments":true,"path":"2023/11/09/2023_09_27_del/","link":"","permalink":"https://hbyecoding.github.io/2023/11/09/2023_09_27_del/","excerpt":"","text":"\\documentclass[a4paper,12pt]{ctexart}\\usepackage{geometry}\\usepackage{amsmath}\\usepackage{amssymb}\\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}\\usepackage[ruled, linesnumbered, vlined]{algorithm2e}\\title{概率统计 作业2}\\date{}\\begin{document}\\maketitle \\textbf{共7题10问, 每问5分, 总计50分.} 在课上讨论的赌徒的例子中, 假设$X_0$有未知的初始分布使得\\begin{equation}q_i = \\mathbb{P}(X_0 = i) &gt; 0, \\quad i = 0, \\cdots, N.\\end{equation}(仅仅为方便讨论, 最终结论并不依赖于各$q_i$的值.) 假设游戏可以一直继续下去, 事件\\begin{equation}A_k = \\{\\omega \\in \\Omega: \\inf \\{ n \\ge k: X_n(\\omega) = 0\\} &gt; \\inf \\{ n\\ge k: X_n(\\omega) = N\\} \\}, \\quad k = 1, 2.\\end{equation}事件$A_k$代表了``从第$k$轮游戏结束后开始算, 赌徒在输光之前持有过$N$枚硬币’’, 而待求的概率\\begin{equation}f_i = \\mathbb{P}(A_1 | X_0 = i).\\end{equation}由于$X_n$为马氏链, 事件$A_1$只取决于$X_1, X_2, \\cdots$, 故概率$f_i$\\emph{不依赖于}$X_0$的具体分布.考虑事件$A_2$与$A_1$的关系:\\begin{equation}\\mathbb{P}(A_2 | X_1 = j, X_0 = i) = \\mathbb{P}(A_2 | X_1 = j) = \\mathbb{P}(A_1 | X_0 = j).\\end{equation}其中, 第一个等号成立的原因在于马氏链的性质(事件$A_2$只取决于$X_2, X_3, \\cdots$), 而第二个等号成立的原因在于将第一次(独立的)游戏除去以后的问题与原问题等价. 利用全概率公式等已有知识, 证明关于$f_i$的等式(即课件中的(18))\\begin{equation}f_i = \\sum_{j=0}^{N} p_{ij} f_j, \\quad i=1,\\cdots, N-1,\\end{equation}其中$p_{ij}$为马氏链$X_n$的转移概率. \\indent 设随机变量$X \\sim \\mathcal{N}(0, 1)$, 随机变量$Z$服从如下离散分布:\\begin{equation}\\mathbb{P}(Z=-1) = \\mathbb{P}(Z=1) = \\frac{1}{2}.\\end{equation}设随机变量$Y$为$X$与$Z$的乘积.\\begin{enumerate}\\item [(1)] 计算出$Y$的累积分布函数或者概率密度函数.\\item [(2)] 计算出$X$与$Y$的协方差.\\end{enumerate}注: 由于事件$X \\in (-1, 1)$与事件$Y \\in (-1, 1)$相同, 而且其概率在区间$(0, 1)$之内, 故$X$与$Y$不独立. \\indent 已知$\\rho \\in (-1, 1)$, $X_1$和$X_2$为独立的服从标准正态分布的随机变量.设\\begin{align}Y_1 &amp;= \\sqrt{\\frac{1+\\rho}{2}} X_1 + \\sqrt{\\frac{1-\\rho}{2}} X_2, \\\\Y_2 &amp;= \\sqrt{\\frac{1+\\rho}{2}} X_1 - \\sqrt{\\frac{1-\\rho}{2}} X_2.\\end{align}\\begin{enumerate}\\item [(1)] 通过计算$X_1$和$X_2$的联合分布, 证明随机向量$\\vec{X} = (X_1, X_2)$服从二维标准多元正态分布.(由此可以推出, 随机向量$\\vec{Y} = (Y_1, Y_2)$服从二维多元正态分布.)\\item [(2)] 计算出随机向量$ \\vec{Y} = (Y_1, Y_2)$服从的分布的概率密度函数.\\end{enumerate} \\indent 设$X_1$和$X_2$为独立且服从$(0,1)$上均匀分布的随机变量.考虑Box—Muller变换:\\begin{align}Y_1 &amp;= \\sqrt{-2 \\ln X_1} \\cos (2 \\pi X_2), \\\\Y_2 &amp;= \\sqrt{-2 \\ln X_1} \\sin (2 \\pi X_2).\\end{align}用$Y_1$和$Y_2$表示出$X_1$和$X_2$, 并计算出雅可比行列式\\begin{equation}\\frac{\\partial(X_1, X_2)}{\\partial(Y_1, Y_2)}\\end{equation}从而验证$(Y_1, Y_2)$服从二维标准多元正态分布. \\indent 设$X_1, \\cdots, X_n$为独立同分布的\\emph{随机变量}, 它们的期望为$\\mu$, 方差为$\\sigma^2$.设\\begin{equation}\\bar{X} = \\frac{1}{n}(X_1 + \\cdots + X_n),\\end{equation}\\begin{equation}T = (X_1 - \\bar{X})^2 + \\cdots + (X_n - \\bar{X})^2.\\end{equation}计算出$\\mathbb{E}[T]$. \\indent 设$T &gt; 0$, $W_t$为定义在$[0, T]$上的布朗运动.布朗桥(Brownian bridge)是一类高斯过程, 通常被用于描述当$W_t$在一段时间区间的首尾被``限定为固定值’’以后的随机过程.设\\begin{equation}B_t = W_t - \\frac{t}{T} W_T, \\quad t \\in [0, T].\\end{equation}显然$B_0 = B_T = 0$, 且$\\mathbb{E}[B_t] = 0$.对$s, t \\in [0, T]$, (利用布朗运动$W_t$的性质)计算出\\begin{equation}K(s, t) = \\mathrm{Cov}(B_s, B_t).\\end{equation}注: 不论是将一个随机过程在未来时刻$T$处的取值限制为$0$, 还是通过未来时刻的$W_T$来定义出$B_t$, 都是反直觉的, 但上述定义方式足够反映出布朗桥的规律. 布朗桥有其他的与之等价的定义方式. \\indent 在Metropolis—Hasting算法中(参见课件), 设对任意的状态$i$和$j$都成立$q_{ij} &gt; 0$.\\begin{enumerate}\\item [(1)] 计算出$X_n$的转移概率\\begin{equation}p_{ij} = \\mathbb{P}(X_{n+1} = j | X_n = i).\\end{equation}\\item [(2)] 证明分布$\\pi$关于转移概率$p_{ij}$满足细致平衡条件.\\end{enumerate} \\end{document}","categories":[],"tags":[]},{"title":"六级级别美词美句","slug":"2023-09-23-vocabulary-sentences","date":"2023-09-23T07:36:18.000Z","updated":"2023-11-09T14:29:59.706Z","comments":true,"path":"2023/09/23/2023-09-23-vocabulary-sentences/","link":"","permalink":"https://hbyecoding.github.io/2023/09/23/2023-09-23-vocabulary-sentences/","excerpt":"","text":"美句 No matter what happens, he always has an air of detachment.(漠然的样子) This preoccupation inevitably leads to an old debate: whether nature or nurture moulds us more.这种关注不可避免地导致了一场熟悉的辩论：天性和后天培养哪一个塑造我们更多。 These 14 million children most urgently need to develop the resilience that is nurtured with play.这1400万儿童极其迫切需要通过玩耍从而培养他们的韧性 Aisles of marketing genius belie the fact that, ultimately, weight loss is dictated by the laws of arithmetic.药店过道里的营销天才掩盖了一个事实，即减肥最终是由算术法则决定的。 There’s not a scrap of evidence to support his claim.没有丝毫证据支持他的说法 真题长难句 Following the Harvard scandal, Mary Miller, the former dean of students at Yale, made an impassioned appeal to her school’s professors to refrain from take-home exams. “It sets such an awful precedent for the future of performance.” Her father, Robin Williams, who died in 2014, was keen to avoid the same fate. But a passing reference in a recent profile by Glamour magazine, to her 20-hour workdays prompted an outpouring of admiration.引发了人们的倾佩之情(的倾泻) Doing a poor job communicating science to the public will only create confusion and widen the gap between science and society, a gap that scientists are trying to close. (n.困惑，糊涂，迷茫) It is text that enables us to uncover lies, confusions and overgeneralizations, and to detect abuses of logic and common sense.正是文本让我们能够发现谎言、混淆和过度概括，并察觉逻辑和常识的滥用。 One government proposition is to eliminate $10,000 of debt for ‘economically distressed’ students. Some UK cities have lobbied without success for the power to levy a charge on visitors.一些英国城市为取得向游客收费的权力进行了游说，但没成功。 It’s Tiffanie Standard, a counselor for young women of color in Philadelphia who want to be tech entrepreneurs — but who must work multiple jobs to stay afloat.蒂芙尼·斯坦达德是费城的一位顾问，为那些想成为科技企业家但又不得不身兼数职以求生存的有色人种年轻女性服务。 We’re allocating too many resources to disciplines that don’t match the needs of employers.我们将太多的资源分配到与雇主需求不匹配的学科上。 But the investigators could not find any data to indicate which species can remember a greater number of distinct human voices, so it was impossible to compare the two on that front.但研究人员找不到任何数据来表明哪些物种记住的不同人类声音更多，所以不可能在这方面对两者进行比较。 For the remainder, the risk of cancer, while increased at the end of life, must be balanced with other factors like remaining life expectancy.对于其他人来说，癌症风险在生命晚期可能增加，而其他因素（比如预期寿命）必须综合起来进行考虑。 Less than half of all American adults were proficient readers in 2017.2017年，只有不到一半的美国成年人是熟练的阅读者。 They give priority to relationships, as careers often recede.事业的重要性往往会逐渐降低，他们把处理各种关系放在了重要位置。 There are queues to climb Mt. Jolmo Lungma as well as to see famous paintings. Leisure, thus conceived; is hard labour, and returning to work becomes a well-earned break from the ordeal.人们排着队去攀登珠穆朗玛峰，欣赏名画。如此想来，休闲是一种艰苦的劳动，而重返工作岗位则成了摆脱磨难的一种应有的休息。 In January 1901, at 1,020 feet, almost precisely the depth predicted by Higgins’ wild guess, the well roared and suddenly ejected mud and six tons of drilling pipe out of the ground, terrifying those present.1901年1月，在1020英尺的深处——几乎与希金斯的大胆预测完全吻合——油井轰鸣着，泥浆和6吨重的钻杆突然喷出地面，吓坏了在场的人。 There appears little prospect of this desperate situation being remedied without radical action being taken.在不采取激进行动的情况下，这种绝望的局面似乎不太可能得到补救。 We received a fast and quick response from the Balinese people. Not only positive responses from the Balinese, we received good responses from the central government, other local governments and even from overseas. Koster told the Sydney Morning Herald this week during an interview.在本周的一次采访中，科斯特告诉《悉尼先驱晨报》：“我们从巴厘岛人民那里得到了快速的回应。我们不仅从巴厘岛人那里收到了积极回应，还从中央政府、其他地方政府甚至是海外收获了良好的回应。” This criticism may ultimately discourage people from doing good deeds.这种批评最终可能会阻碍人们做好事。 词汇recedevi. 退，退去，渐渐远去；向后倾斜；缩进；（特点，感觉）逐渐减少；（可能性）变得渺茫 trenchn. 沟，沟渠；战壕；海沟 primary adj. 首要的，主要的；最初的；原本的；第一手的, 直接的；初级的, 小学的；初等的gorgeousadj. 华丽的，灿烂的；令人愉快的impair vt. 损害，损伤；削弱 peg n. 小钉，栓，挂物钉；桩 vt. 用钉子钉，用钉（或桩等）固定；限定（价格、工资等）；将…看成，将…归入 violate [‘vaɪəleɪt] vt. 违反；侵犯，妨碍；亵渎volatile [‘vɒlətaɪl] n. 挥发物；有翅的动物 adj. [化学] 挥发性的；不稳定的；爆 benevolencen.仁爱, 善行, 厚道, 捐助","categories":[],"tags":[]},{"title":"如果重回大学, 你的计算机学习生涯如何度过?","slug":"2023-09-23-CS-thoughts","date":"2023-09-23T04:31:52.000Z","updated":"2023-11-09T14:29:59.944Z","comments":true,"path":"2023/09/23/2023-09-23-CS-thoughts/","link":"","permalink":"https://hbyecoding.github.io/2023/09/23/2023-09-23-CS-thoughts/","excerpt":"","text":"数学课程 微积分 高等代数(and 矩阵论) 概率论 复分析 凸优化 计算机课程 c/c++ 算法和数据结构 操作系统 编译原理 计算机网络 数字电路和模拟电路 空闲时间找项目或者赚点小钱 python和react 的web 编程…","categories":[],"tags":[]},{"title":"Matrix-w03","slug":"Matrix-w03","date":"2023-09-23T01:56:27.000Z","updated":"2023-11-09T14:30:04.428Z","comments":true,"path":"2023/09/23/Matrix-w03/","link":"","permalink":"https://hbyecoding.github.io/2023/09/23/Matrix-w03/","excerpt":"","text":"Properties on the Matrix InverseInner Product$ $ = = tr(A^HB)矩阵的共轭转置 Frobenius 范数 Rank of a Matrix max number of linearly independent rows/cols rank(A) = dim row(A) = dim col(A) row(A) = \\{ y^TA\\}col(A) = \\{ Ax\\}rank{ AX} ?? rank(A)什么满秩的时候/左乘一个列满秩 或者 右乘一个 行满秩的矩阵 不改变 A的rank 这里面其实有A = XY (总可以) 未来我们使用这个性质 就是总可以 AB = XY (if AB!= 0) determination of MATRIXproperties of det(A)8 properties AND 2 other properties","categories":[],"tags":[]},{"title":"服务业和KIT","slug":"great-nation-note-0","date":"2023-09-22T10:22:08.000Z","updated":"2023-11-09T14:29:59.885Z","comments":true,"path":"2023/09/22/great-nation-note-0/","link":"","permalink":"https://hbyecoding.github.io/2023/09/22/great-nation-note-0/","excerpt":"","text":"不同于农业和工业, 服务业的产品更具有不可运输性, 大多数服务产品需要面对面地完成.同时现代服务业越来越依靠知识,信息和技术作为核心投入品 关于更细分的消费性服务业消费型服务业是跟着人和钱走的 发展消费型服务业的固定投入更容易被分摊, 生意好做 劳动生产率越高的城市, 富人越多, 人对消费型服务的需求越强, 比如家政服务将这些高知分子或者富人从家务中解放出来, 有利于提升劳动生产率, 促进社会分工 活在城市还是生活在城市?生活在浦西(life is in PUXI) 活在浦东(lay in PUDONG)低密度和高密度之间的区别窄道路适宜行走. 大广场, 宽马路和大的绿化带则不行.城市建设中, 步行和骑车开始取代开车","categories":[],"tags":[]},{"title":"java","slug":"java","date":"2023-09-01T13:03:28.000Z","updated":"2023-11-09T14:30:04.229Z","comments":true,"path":"2023/09/01/java/","link":"","permalink":"https://hbyecoding.github.io/2023/09/01/java/","excerpt":"","text":"how to use javacompile: 1$ javac hello.java hello run: 1$ java hello","categories":[],"tags":[]},{"title":"python","slug":"python","date":"2023-09-01T08:47:28.000Z","updated":"2023-11-09T14:30:04.499Z","comments":true,"path":"2023/09/01/python/","link":"","permalink":"https://hbyecoding.github.io/2023/09/01/python/","excerpt":"","text":"1print(&#x27;hello world&#x27;) hello world a b c a b c","categories":[],"tags":[]},{"title":"最近学习矩阵论, 先把一些重要的性质摘出来","slug":"determinant01","date":"1970-01-01T05:37:11.009Z","updated":"2023-11-09T14:29:59.845Z","comments":true,"path":"1970/01/01/determinant01/","link":"","permalink":"https://hbyecoding.github.io/1970/01/01/determinant01/","excerpt":"","text":"Property 1: The determinant of the identity matrix is one;Property 2: The determinant changes sign under row/column interchange;Property 3: The determinant is a linear function of the first row, holding all other rowsfixed. SVD from EVD要从矩阵$AA^H$的特征值分解（EVD）中获取矩阵A的奇异值分解（SVD），您可以按照以下步骤进行： 计算矩阵$AA^H$的特征值分解（EVD）：首先，计算矩阵$AA^H$的特征值分解。对于Hermitian（复数域中对称的）矩阵$M$，其特征值分解为： M = U \\Lambda U^H其中： $U$是一个酉矩阵，其列是$M$的特征向量。 $\\Lambda$是一个对角矩阵，包含$M$的特征值。 计算矩阵A的奇异值分解（SVD）：一旦您获得了矩阵$AA^H$的特征值分解，您可以如下计算矩阵A的SVD： A = U \\sqrt{\\Lambda} V^H其中： $U$是从$AA^H$的特征值分解中获得的酉矩阵。 $\\sqrt{\\Lambda}$是一个对角矩阵，包含$AA^H$的特征值的平方根（逐元素取平方根）。 $V^H$是矩阵$V$的共轭转置，$V$可以是任何酉矩阵。 因此，基本上，您使用$AA^H$的特征值分解来找到特征值和特征向量，然后用它们构建矩阵A的奇异值分解。A的左奇异向量是$AA^H$的特征向量，奇异值是$AA^H$的特征值的平方根。右奇异向量可以通过关系$V = A^H U \\Lambda^{-1/2}$从左奇异向量和原始矩阵A获得，但请注意，由于SVD的酉性质，右奇异向量并不唯一。 SVD properties“MP inverse based on the SVD” 表示基于奇异值分解（SVD）计算的Moore-Penrose伪逆（MP逆）。 奇异值分解（SVD）是一种将一个矩阵分解为三个矩阵乘积的数学工具，具体而言，将矩阵A分解为以下形式： A = U \\Sigma V^H其中： U是一个酉矩阵（正交矩阵），它的列包含A的左奇异向量。 Σ是一个对角矩阵，其对角线上的元素是A的奇异值。 $V^H$是矩阵V的共轭转置，它的列包含A的右奇异向量。 Moore-Penrose伪逆（MP逆）是矩阵的一种广义逆，可以用来求解线性最小二乘问题以及在矩阵求逆不可行时的一些问题。对于一个矩阵A，它的MP逆通常表示为$A^+$。 “MP inverse based on the SVD” 意味着通过使用A的奇异值分解（SVD），可以计算其Moore-Penrose伪逆$A^+$。这通常涉及到利用SVD的性质来构建MP逆，具体方法是使用U，Σ，$V^H$来计算$A^+$，通常的公式是： A^+ = V \\Sigma^+ U^H其中，Σ^+是Σ的伪逆，即将Σ的非零奇异值取倒数，然后取共轭转置。 这种方法在数值计算和线性代数中非常有用，特别是在处理奇异矩阵（不可逆矩阵）或求解超定系统（过多未知数的线性方程组）时。 矩阵A的伪逆（Moore-Penrose伪逆）记作$A^{+}$，它对于非方阵A是一种广义的逆矩阵。矩阵A的伪逆具有以下几个性质： $AA^{+}A = A$ $A^{+}AA^{+} = A^{+}$ $(AA^{+})^T = AA^{+}$ $(A^{+}A)^T = A^{+}A$ 现在，考虑$x_0 = A^{+}y$，您想知道是否总是可以找到y，使得$y = A^{+}x_0$。让我们进行分析： 从$x_0 = A^{+}y$开始，如果要找到y，可以两边同时左乘A： $Ax_0 = AA^{+}y$ 根据伪逆的性质，我们可以将其简化为： $Ax_0 = (AA^{+})y = y$ 因此，如果$Ax_0 = y$，那么$y = A^{+}x_0$。 然而，有些情况下这个等式可能不成立： 如果矩阵A是奇异矩阵（不满秩），则可能不存在唯一的y解。在这种情况下，A将没有完整的Moore-Penrose伪逆。 如果$Ax_0$不在A的列空间（列空间）内，那么就不存在满足等式$Ax_0 = y$的y。 因此，$y = A^{+}x_0$的存在与矩阵A的性质、$x_0$的具体值以及是否存在满足等式的y解有关。 The statement you’ve provided suggests that if you have a vector x such that the 2-norm (Euclidean norm) of the difference between A times x_0 and y is equal to the 2-norm of the difference between A times x and y (i.e., ‖Ax_0 - y‖_2 = ‖Ax - y‖_2), then the norm of x_0 is less than or equal to the norm of x (‖x_0 ‖_2 ≤ ‖x‖_2), and equality only holds when x_0 is equal to x. This statement is related to the concept of the orthogonal projection. When ‖Ax_0 - y‖_2 = ‖Ax - y‖_2, it means that both x_0 and x minimize the 2-norm of the error between the projected vector A times x (or A times x_0) and the target vector y. In such a case, x_0 and x are both candidates for the solution, but the statement ‖x_0 ‖_2 ≤ ‖x‖_2 implies that x_0 is the solution that minimizes the norm among all possible solutions. If ‖x_0 ‖_2 is less than ‖x‖_2, it means that x_0 is closer to the origin, and therefore has a smaller Euclidean norm. Equality ‖x_0 ‖_2 = ‖x‖_2 only holds when x_0 is equal to x, indicating that x_0 is the optimal solution. If x_0 is not equal to x, it implies that x is further away from the origin than x_0, resulting in a larger Euclidean norm. So, when ‖Ax_0 - y‖_2 = ‖Ax - y‖_2, x_0 is the solution that minimizes the 2-norm of the error, and its norm is either less than or equal to the norm of any other candidate solution x. 要证明 $x = A^{+} y$ 是 ||x|| 的最小解，您可以使用 KKT 条件（Karush-Kuhn-Tucker 条件）来建立证明。在这个情况下，您想要最小化 ||x||，同时满足 $x = A^{+} y$。下面是如何使用 KKT 条件来证明 $x = A^{+} y$ 是 ||x|| 的最小解： 考虑最小化目标函数：f(x) = ||x||（即，f(x) 是 x 的 2-范数）。 我们有一个等式约束：$x = A^{+} y$。 使用 KKT 条件，我们得到以下条件： 梯度条件：∇f(x) - λ(∇(x - A^{+}y)) = 0，其中 λ 是 Lagrange 乘子。 稳定性条件：f(x) 在 x = A^{+}y 处是凸函数。 现在，我们来分析这些条件： 梯度条件：∇f(x) = 2x，∇(x - A^{+}y) = I，其中 I 是单位矩阵。因此，梯度条件变为：2x - λI = 0。 稳定性条件：我们知道 2-范数是一个凸函数，因此 f(x) = ||x|| 在整个定义域上都是凸的。 现在，考虑 Lagrange 乘子 λ 和梯度条件。由梯度条件，我们有： 2x - λI = 0 这意味着 x = (1/2λ)I。 现在，我们回到我们的目标函数 f(x) = ||x||。根据 x 的表达式，我们有： f(x) = ||(1/2λ)I|| = (1/2λ)||I|| = 1/(2λ) 因此，我们看到当 λ 取得最小值时，f(x) = 1/(2λ) 取得最大值。这意味着我们要最小化 ||x||，必须使 λ 取得最小值。而根据 KKT 条件，λ 必须是非负的。因此，λ 最小值为 0。 当 λ = 0 时，我们得到 x = (1/2λ)I = (1/2*0)I = 0I = 0。这表明 x = A^{+}y 是 ||x|| 的最小解，因为它使目标函数取得最小值 0。 因此，我们已经证明 x = A^{+}y 是 ||x|| 的最小解。 设 X_1, …, X_n 是从正态分布 $N(\\mu, \\sigma^2)$ 的总体中抽出的样本. 如果始终假设 n 为奇数, 则可以使用样本的中位数 ˜\\mun 作为 \\mu 的估计量, 这一估计量是渐近正态的, 满足当 n → ∞ 时，√n(˜\\mun − \\mu) (1)依分布收敛到 N (0,π2σ2). 而极大似然估计可以给出方差渐近地最优的结果.(1) 推导出 \\mu 的极大似然估计 ˆ\\mun 的表达式.(2) 写出当 n → ∞ 时,√n(ˆ\\mun − \\mu) 收敛到的分布. 要进行假设检验问题 H0: σ = σ0 vs. H1: σ ≠ σ0，你可以仿照 Wald 检验，但使用卡方分布来替代标准正态分布，具体的步骤如下： 提出原假设 (H0) 和备择假设 (H1)： 原假设 (H0): σ = σ0 备择假设 (H1): σ ≠ σ0 收集样本数据，计算样本标准差 (s)。 计算检验统计量 (Wald 统计量)： Wald 统计量的计算方式为：W = [(n - 1) * s^2] / σ0^2其中，n 是样本容量，s 是样本标准差，σ0 是原假设中的标准差。 基于卡方分布 (χ^2) 进行检验： 在这个情况下，Wald 统计量 W 满足自由度为 n - 1 的卡方分布，即 W ~ χ^2(n - 1)。 选择显著性水平 (α). 根据所选择的显著性水平 (α) 和自由度 (n - 1) 查找卡方分布表格或使用统计软件，确定卡方临界值 (χ^2_critical)，使得 P(χ^2 &gt; χ^2_critical) = α/2。 判断是否拒绝原假设: 如果 W &gt; χ^2_critical 或 W &lt; 1 / χ^2_critical，则拒绝原假设 (H0). 如果 W ≤ χ^2_critical 并 W ≥ 1 / χ^2_critical，则无法拒绝原假设 (H0). 根据判断的结果，得出对标准差 σ 的结论: 如果拒绝原假设，说明在所选显著性水平下，有足够的证据表明标准差 σ 不等于 σ0. 如果未能拒绝原假设，说明在所选显著性水平下，没有足够的证据表明标准差 σ 与 σ0 不同。 这样，你就可以使用卡方分布来进行关于标准差 σ 的假设检验，检验是否存在显著差异。 To prove that for any non-singular matrix A, there exists an upper-triangular matrix T such that TA is a unitary matrix, we can use the concept of the QR decomposition. QR decomposition decomposes any matrix A into a product of two matrices: Q (orthogonal or unitary) and R (upper triangular). Specifically: A = QR Here, Q is unitary, and R is upper triangular. Now, let’s prove the statement for a non-singular matrix A: Given a non-singular matrix A, perform the QR decomposition: A = QR Here, Q is unitary, and R is upper triangular. We want to show that TA is unitary: TA = TQR Since Q is unitary, we have: Q^H Q = I (Where Q^H is the conjugate transpose of Q, and I is the identity matrix). Now, let’s define T = Q^H. Since Q is unitary, we have: T = Q^H Then, TA becomes: TA = (Q^H) QR Since T = Q^H and Q^H Q = I, we have: TA = IR Any upper triangular matrix multiplied by its own conjugate transpose (in this case, R and R^H) will result in the identity matrix times a unitary matrix (IR), which is unitary itself. So, TA is a unitary matrix, and we have successfully shown that for any non-singular matrix A, there exists an upper-triangular matrix T (in this case, T = Q^H) such that TA is a unitary matrix. Prove : If A is a real matrix, and $A + A^T = 0$, then $(I - A)(I + A)^{-1}$ is an orthogonal matrix\\\\ Let $A$ be a square matrix of order $n$ and $P_A(x)$ be the characteristic polynomial of $A$ defined as: $P_A(x) = \\det(A - xI)$, We want to show that $P_A(A) = 0$. Substituting $x$ with the matrix $A$ in the characteristic polynomial, we get: $P_A(A) = \\det(A - AI)$. Now, we use the properties of determinants: $\\det(A - AI) = \\det(A(I - A))$. We can factor out $A$: $\\det(A(I - A)) = \\det(A) \\det(I - A)$. Since $A$ and $I - A$ have the same size, $\\det(I - A)$ is a scalar, and we can write it as $c$: $\\det(A) \\cdot c$. We need to show that $\\det(A) \\cdot c = 0$. Since $c$ is a scalar, it can be written as $c = c^n$ (for a $n \\times n$ matrix). Now, we have: $\\det(A) \\cdot c = \\det(A) \\cdot c^n$. We know that the determinant of a matrix raised to its order is always zero (if it’s not invertible): $\\det(A^n) = 0$. Therefore, we have: $\\det(A) \\cdot c = 0$. This concludes the proof that Hamilton-Cayley theorem holds: $P_A(A) = \\det(A - AI) = \\det(A) \\det(I - A) = \\det(A) \\cdot c = 0$.","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"test","slug":"test","permalink":"https://hbyecoding.github.io/tags/test/"}]}